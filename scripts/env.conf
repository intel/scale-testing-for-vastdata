#!/bin/bash

# Intel Copyright © 2021, Intel Corporation.
# SPDX-License-Identifier: BSD-3-Clause

network_speed=${1:-"10"}
step_size=${2:-"0"}

#=================== Input validation ============================================================#
source="${BASH_SOURCE}"
scripts_dir="$(cd "$(dirname "$source")" || exit; pwd)"
source "$scripts_dir/common.conf" 

if [ "$network_speed" != "10" ] && [ "$network_speed" != "100" ]; then
    echo "Invalid network speed specified, must be in [10, 100]."
    exit 22 # Standard EINVAL for Invalid ArgumentL
fi

#==================== User Defined Dummy Variables, Update Accordingly ===========================#

# Update with the actual IPs and client cluster hostnames.

# The test harness assume the VIPs are always contiguous.
#
# Client node IPs don't have to be contiguous, ie it can be client-cluster[05,10-20,50-55]
# The number of client nodes can be any number >= 1. The lead node(FIO client) shall not be
# in the list.

_VIP_POOL_10G="10.A.B.110-10.A.B.157"
_VIP_POOL_100G="192.168.B.110-192.168.B.157"
_CLIENT_NODES_10G="client-cluster[01-100]"
_CLIENT_NODES_100G="client-cluster[01-20]"

# Replace the key and secret with the valid ones
ELBENCHO_S3KEY="YU5...LQ1"
ELBENCHO_S3SECRET="aqb...EtnQ"

# For result upload to CIFS share on remote Windows server, 
# update needed only if RESULTS_CIFS_UPDATE_ENABLE is 1.
SHAREPOINT_HOST="YourSharepointHostName"
SHAREPOINT_FOLDER="//${SHAREPOINT_HOST}/YourRemoteCifsSharePathOnSharepointHost"

#==================== User Defined Tunnable Variables, Configure Accordingly ===========================#

# Intel Turbo Boost control
TURBO_BOOST_ENABLE=0

# Hyper Threading control
HYPER_THREADING_ENABLE=0

# irqbalance control
IRQBALANCE_ENABLE=0

# Nodes can be defined as ordered or unordered.
# 
# 1. Ordered nodes can be defined in format of:
#    a) node[01-02,30-32,10-12], or
#    b) node{01..02} node{30..32} node{10..12}
# Nodes will be expanded to "node01 node02 node10 node11 node12 node30 node31 node32" with nodeset --expand

# 2. Unordered nodes SHALL be defined in format of:
#    node{01..02} node{30..32} node{10..12}
# Nodes will be expanded to "node01 node02 node30 node31 node32 node10 node11 node12" with eval only
ORDERED_NODES=1

# To enable/disable FIO Server/Client mode.
# 1: to enable FIO Server/Client mode. 
#     Aggregation: results are aggregated in fio json+ output as "All_Clients"
#     Where:       head node "results" folder
# 0: to disable FIO Server/Client mode.
#     Aggregation: no result aggregation of all clients, json+ output is per client.
#     Where:       in $HOME/$VAST_SCALE_TESTING_PATH folder
# For better total BW/IOPS/Latency tracking, use Server/Client mode.

FIO_SERVER_CLIENT_MODE=1

# Mount configurations
# Kernel dependent Multipath driver is needed in order to enable this. 
# Contact support@vastdata.com for it if needed. 
# Enabling Multipath allows more balanced load on VAST Data CNodes and DNodes,
# especially when worker/thread count is low.
#
#When the variable is set to 0, VIPs are going to be mounted individually. 
MULTIPATH_ENABLE=1

# Applicable to FIO and  MULTIPATH_ENABLE=0 only.
# 1: 1 mount per VIP per client node. Overall mounts per client = VIP number. 
#    Mounts are created before any workloads start. 
#
#    This is a preferred way of mounting, as it’s faster and no noticeable 
#    performance penalty due to over mount, i.e mount number > job/worker number. 
#
# 0: 1 mount per job/thread per client node. Overall mounts per client = job/thread number. 
#    Mounts are created on the fly when running workload. This could be slower.
MOUNT_ALL=1

# This is only applicable to MULTIPATH_ENABLE=0 and MOUNT_ALL=0 . 
# This is to avoid creating too many clush sessions when doing individual mounts on the fly.       
MOUNT_STAGGER=5

# The mountpoint on client system when Multipath is enabled.
MOUNT_MULTIPATH_ENABLED="/mnt/nfs-multipath-tcp"

# The mountpoint on client system when Multipath is disabled.
MOUNT_MULTIPATH_DISABLED="/mnt/nfs-no-multipath"

# NConnect allows multiple TCP connections to be created for each mount, 
# hence improves the BW effectively. This is independent of Multipath, 
# and can be used together with Multipath.
# NCONNECT_ENABLE must be set to 1 for this to take effect.
NCONNECT_NUM=48

# When MULTIPATH_ENABLE=0, and NCONNECT_ENABLE=1, big NCONNECT_NUM means large number of TCPs 
# will be created per client as the table below shows. This could: 
#    (1) take a long time to mount before a test can start 
#    (2) cause connection aborts if too many contentions. Best practice is to use:
#            MULTIPATH_ENABLE=1
#            NCONNECT_ENABLE=1
#        Or:
#            MULTIPATH_ENABLE=0
#            NCONNECT_ENABLE=0
#
#     Per Client TCP Connections
#     ===============================================================
#     MULTIPATH_ENABLE	| NCONNECT_ENABLE	| Per Client TCPs
#     ---------------------------------------------------------------
#     1	                |1	              |NCONNECT_NUM
#     1	                |0	              |1
#     0	                |0	              |VIP Count
#     0	                |1	              |VIP Count * NCONNECT_NUM
#     ---------------------------------------------------------------
NCONNECT_ENABLE=1

# Folder to be created under VIP root for FIO, elbencho file and object testings
REMOTE_PATH_FIO="fio"
REMOTE_PATH_ELBENCHO_FILE="elbencho-file"
REMOTE_PATH_ELBENCHO_S3="elbencho-s3"

# If FIO is installed in /usr/local/bin or other path, change FIO_PATH accordingly
FIO_PATH="/usr/bin"

# Elbencho definitions
ELBENCHO_ID="6ac5b651eaa9"  # Use "docker image ls" to find it on lead node after the docker pull
ELBENCHO_TAG="scaletest01"  # Change the tag after a new deployment to help identify the image
ELBENCHO_IMG="elbencho.$ELBENCHO_TAG" # Full image identifier by tag
# This defines the folder to store the elbencho image on head and client nodes.
ELBENCHO_IMAGE_FOLDER="$VAST_SCALE_TESTING_PATH/elbencho"

# Disabled by default.
# To enable this, make sure the sharepoint is set up properly
RESULTS_UPLOAD_ENABLE=0
# NFS share by default. The entire mount command can be redefined in case other type of
# mount is preferred. Note the the command must start with "sudo mount".
RESULTS_UPLOAD_MOUNT_CMD="sudo mount -t nfs ${SHAREPOINT_FOLDER} ${RESULTS_MNTPOINT}"

# General test controls
INTER_WORKLOAD_WAIT_IN_SECONDS=60
INTER_STEP_WAIT_IN_SECONDS=120

# CentOS default 64 is not optimal for scale testing with 100 client nodes, 
# as nodes beyond the threshold will be staggered until the previous batch is done, 
# causing undesired synchronization issues.
CLUSH_MAX_FAN_OUT=120 
    
# Put your own values in env.conf.override to:
#   - replace the dummy variables (must have)
#   - replace any tunable variables if desired (optional). 
# Ensure all dependencies of the variable above this shall be updated as well.
#
# env.conf.override is ignored on commit by .gitignore, 
# you don't want to push them to the public repo
_customized_config="$SCRIPT_DIR/env.conf.override"
if [ -f "$_customized_config" ]; then
    # shellcheck source=/dev/null
    source "$_customized_config"
fi
  
#==================== Derived Variables, No Changes Required ==================================#

HEAD_NODE="$(hostname -s)"

CLUSTER_DOMAIN_NAME=$(hostname | sed "s/$HEAD_NODE.//g")

_HOST_SUBNET_FIRST_VIP_10G=$(echo "$_VIP_POOL_10G" | cut -d'-' -f 1)
_HOST_SUBNET_LAST_VIP_10G=$(echo "$_VIP_POOL_10G" | cut -d'-' -f 2)
_HOST_SUBNET_FIRST_VIP_LAST_FIELD_10G=$(echo "$_HOST_SUBNET_FIRST_VIP_10G" | cut -d'.' -f 4)
_HOST_SUBNET_LAST_VIP_LAST_FIELD_10G=$(echo "$_HOST_SUBNET_LAST_VIP_10G" | cut -d'.' -f 4)
_HOST_SUBNET_10G=$(echo "$_HOST_SUBNET_FIRST_VIP_10G" | cut -d'.' -f 1-3)

_HOST_SUBNET_FIRST_VIP_100G=$(echo "$_VIP_POOL_100G" | cut -d'-' -f 1)
_HOST_SUBNET_LAST_VIP_100G=$(echo "$_VIP_POOL_100G" | cut -d'-' -f 2)
_HOST_SUBNET_FIRST_VIP_LAST_FIELD_100G=$(echo "$_HOST_SUBNET_FIRST_VIP_100G" | cut -d'.' -f 4)
_HOST_SUBNET_LAST_VIP_LAST_FIELD_100G=$(echo "$_HOST_SUBNET_LAST_VIP_100G" | cut -d'.' -f 4)
_HOST_SUBNET_100G=$(echo "$_HOST_SUBNET_FIRST_VIP_100G" | cut -d'.' -f 1-3)

NETWORK_SPEED="$network_speed"
_HOST_SUBNET_NAME=_HOST_SUBNET_"${NETWORK_SPEED}"G
eval HOST_SUBNET="\$$_HOST_SUBNET_NAME"

_HOST_SUBNET_FIRST_VIP_NAME=_HOST_SUBNET_FIRST_VIP_"${NETWORK_SPEED}"G
eval HOST_SUBNET_FIRST_VIP="\$$_HOST_SUBNET_FIRST_VIP_NAME"

_HOST_SUBNET_LAST_VIP_NAME=_HOST_SUBNET_LAST_VIP_"${NETWORK_SPEED}"G
eval HOST_SUBNET_LAST_VIP="\$$_HOST_SUBNET_LAST_VIP_NAME"

_HOST_SUBNET_FIRST_VIP_LAST_FIELD_NAME=_HOST_SUBNET_FIRST_VIP_LAST_FIELD_"${NETWORK_SPEED}"G
eval HOST_SUBNET_FIRST_VIP_LAST_FIELD="\$$_HOST_SUBNET_FIRST_VIP_LAST_FIELD_NAME"

_HOST_SUBNET_LAST_VIP_LAST_FIELD_NAME=_HOST_SUBNET_LAST_VIP_LAST_FIELD_"${NETWORK_SPEED}"G
eval HOST_SUBNET_LAST_VIP_LAST_FIELD="\$$_HOST_SUBNET_LAST_VIP_LAST_FIELD_NAME"

ALL_VIPS=()
for (( ip = "$HOST_SUBNET_FIRST_VIP_LAST_FIELD"; ip <= "$HOST_SUBNET_LAST_VIP_LAST_FIELD"; ip++ )) 
do 
    ALL_VIPS+=("$ip")
done  
    
_CLIENT_NODES_COMPACTED_NAME="_CLIENT_NODES_${NETWORK_SPEED}G"
eval _CLIENT_NODES_COMPACTED="\$$_CLIENT_NODES_COMPACTED_NAME"

if [ "$_CLIENT_NODES_COMPACTED" == "" ]; then
    _CLIENT_NODES_STRING=""
else    
    _CLIENT_NODES_STRING=$(eval echo "$_CLIENT_NODES_COMPACTED")
    if [ "$ORDERED_NODES" == "1" ]; then
        _CLIENT_NODES_STRING=$(nodeset --expand "$_CLIENT_NODES_STRING")
    fi  
fi

CLIENT_NODES_ARRAY=($_CLIENT_NODES_STRING)

CLIENT_NODES=${_CLIENT_NODES_STRING// /,}
   
_ALL_NODES_STRING="$HEAD_NODE $_CLIENT_NODES_STRING"
ALL_NODES_ARRAY=($_ALL_NODES_STRING)

CLIENT_COUNT=$(echo "$_CLIENT_NODES_STRING" | wc --word)

if [ "$step_size" -le "0" ] || [ "$step_size" -gt "$CLIENT_COUNT" ]; then
    step_size="$CLIENT_COUNT"
fi
STEP_SIZE="$step_size"

if [ "$CLIENT_COUNT" -eq "0" ]; then
    export STEPS=0
else
    export STEPS=$(((CLIENT_COUNT + STEP_SIZE - 1) / STEP_SIZE))
fi
